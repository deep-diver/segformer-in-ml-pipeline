{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "2XC0gW0lrNLE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "htR5VyTB3EMw",
        "outputId": "206c3284-a04b-45ff-be53-037cf52a6a85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 6.1 MB 29.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 63.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 270 kB 67.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 72.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 57.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 50.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 80 kB 9.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 68 kB 6.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 856 kB 65.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 594 kB 51.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 57.3 MB/s \n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gradio huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch a model from the HF Hub"
      ],
      "metadata": {
        "id": "30-khJ8IrPFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import from_pretrained_keras\n",
        "\n",
        "\n",
        "model_ckpt = \"chansung/test-unet\"\n",
        "\n",
        "MODEL = from_pretrained_keras(model_ckpt)"
      ],
      "metadata": {
        "id": "-2iyn7KSqAQ6",
        "outputId": "0d6323f1-ffd1-4dd7-f677-e973e708850a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "config.json not found in HuggingFace Hub\n",
            "WARNING:huggingface_hub.hub_mixin:config.json not found in HuggingFace Hub\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch a test image"
      ],
      "metadata": {
        "id": "cvRWR0perRbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://i.ibb.co/whmGJr4/test-image.jpg -O test-image.jpg"
      ],
      "metadata": {
        "id": "H7brO_TxqaQ7",
        "outputId": "90c004a2-c88e-4aae-e3ed-c3fce6e0bb56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-13 02:25:21--  https://i.ibb.co/whmGJr4/test-image.jpg\n",
            "Resolving i.ibb.co (i.ibb.co)... 51.210.32.106, 217.182.228.53, 51.210.32.103, ...\n",
            "Connecting to i.ibb.co (i.ibb.co)|51.210.32.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 182420 (178K) [image/jpeg]\n",
            "Saving to: ‘test-image.jpg’\n",
            "\n",
            "test-image.jpg      100%[===================>] 178.14K   505KB/s    in 0.4s    \n",
            "\n",
            "2022-09-13 02:25:22 (505 KB/s) - ‘test-image.jpg’ saved [182420/182420]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ],
      "metadata": {
        "id": "RGZboR5urTFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image \n",
        "import numpy as np \n",
        "import tensorflow as tf \n",
        "\n",
        "\n",
        "RESOLTUION = 128\n",
        "# MODEL = tf.keras.models.load_model(model_path)\n",
        "\n",
        "\n",
        "def preprocess_input(image: Image) -> tf.Tensor:\n",
        "    image = np.array(image)\n",
        "    image = tf.convert_to_tensor(image)\n",
        "\n",
        "    image = tf.image.resize(image, (RESOLTUION, RESOLTUION))\n",
        "    image = image / 255\n",
        "\n",
        "    return tf.expand_dims(image, 0)\n",
        "\n",
        "\n",
        "# The below utilities (sidewalk_palette(), get_seg_overlay()) are from:\n",
        "# https://github.com/deep-diver/semantic-segmentation-ml-pipeline/blob/main/notebooks/inference_from_SavedModel.ipynb\n",
        "\n",
        "def sidewalk_palette():\n",
        "    \"\"\"Sidewalk palette that maps each class to RGB values.\"\"\"\n",
        "    return [\n",
        "        [0, 0, 0],\n",
        "        [216, 82, 24],\n",
        "        [255, 255, 0],\n",
        "        [125, 46, 141],\n",
        "        [118, 171, 47],\n",
        "        [161, 19, 46],\n",
        "        [255, 0, 0],\n",
        "        [0, 128, 128],\n",
        "        [190, 190, 0],\n",
        "        [0, 255, 0],\n",
        "        [0, 0, 255],\n",
        "        [170, 0, 255],\n",
        "        [84, 84, 0],\n",
        "        [84, 170, 0],\n",
        "        [84, 255, 0],\n",
        "        [170, 84, 0],\n",
        "        [170, 170, 0],\n",
        "        [170, 255, 0],\n",
        "        [255, 84, 0],\n",
        "        [255, 170, 0],\n",
        "        [255, 255, 0],\n",
        "        [33, 138, 200],\n",
        "        [0, 170, 127],\n",
        "        [0, 255, 127],\n",
        "        [84, 0, 127],\n",
        "        [84, 84, 127],\n",
        "        [84, 170, 127],\n",
        "        [84, 255, 127],\n",
        "        [170, 0, 127],\n",
        "        [170, 84, 127],\n",
        "        [170, 170, 127],\n",
        "        [170, 255, 127],\n",
        "        [255, 0, 127],\n",
        "        [255, 84, 127],\n",
        "        [255, 170, 127],\n",
        "    ]\n",
        "\n",
        "\n",
        "def get_seg_overlay(image, seg):\n",
        "    color_seg = np.zeros(\n",
        "        (seg.shape[0], seg.shape[1], 3), dtype=np.uint8\n",
        "    )  # height, width, 3\n",
        "    palette = np.array(sidewalk_palette())\n",
        "    \n",
        "    for label, color in enumerate(palette):\n",
        "        color_seg[seg == label, :] = color\n",
        "\n",
        "    # Show image + mask\n",
        "    img = np.array(image) * 0.5 + color_seg * 0.5\n",
        "\n",
        "    img *= 255\n",
        "    img = np.clip(img, 0, 255)\n",
        "    img = img.astype(np.uint8)\n",
        "    return img\n",
        "\n",
        "\n",
        "def run_model(image: Image) -> tf.Tensor:\n",
        "    preprocessed_image = preprocess_input(image)\n",
        "    prediction = MODEL.predict(preprocessed_image)\n",
        "\n",
        "    seg_mask = tf.math.argmax(prediction, -1)\n",
        "    seg_mask = tf.squeeze(seg_mask)\n",
        "    return seg_mask\n",
        "\n",
        "\n",
        "def get_predictions(image: Image):\n",
        "    predicted_segmentation_mask = run_model(image)\n",
        "    preprocessed_image = preprocess_input(image)\n",
        "    preprocessed_image = tf.squeeze(preprocessed_image, 0)\n",
        "    \n",
        "    pred_img = get_seg_overlay(preprocessed_image.numpy(), predicted_segmentation_mask.numpy())\n",
        "    return Image.fromarray(pred_img)"
      ],
      "metadata": {
        "id": "db8dneFo40vk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio demo"
      ],
      "metadata": {
        "id": "y2ckYHmIrWXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "title = \"Simple demo for a semantic segmentation model trained on the Sidewalks dataset.\"\n",
        "\n",
        "description = \"\"\"\n",
        "\n",
        "Note that the outputs obtained in this demo won't be state-of-the-art. The underlying project has a different objective focusing more on the ops side of\n",
        "deploying a semantic segmentation model. For more details, check out the repository: https://github.com/deep-diver/semantic-segmentation-ml-pipeline/.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    get_predictions,\n",
        "    gr.inputs.Image(type=\"pil\"),\n",
        "    \"pil\",\n",
        "    allow_flagging=\"never\",\n",
        "    title=title,\n",
        "    description=description,\n",
        "    examples=[[\"test-image.jpg\"]]\n",
        ")\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "UdbLNOsa8BSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To add more examples, do - `examples=[[\"test-image.jpg\"], [\"image2.jpg\"], [\"sample.png\"], ...]`."
      ],
      "metadata": {
        "id": "JnxH5QmurAhq"
      }
    }
  ]
}